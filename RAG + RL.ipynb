{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b40c207c-0235-4a8e-84d0-0c1e04b4e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import sacrebleu\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f152e-a0f0-4527-83f3-70064dd4f1d8",
   "metadata": {},
   "source": [
    "## RAG from Scatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d959fda0-2e6e-4a3c-b50a-c8b9c14db160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document downloaded and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.gutenberg.org/files/1342/1342-0.txt'\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  \n",
    "\n",
    "document_content = response.text\n",
    "\n",
    "with open('pride_and_prejudice.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(document_content)\n",
    "\n",
    "print(\"Document downloaded and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2de03d89-e862-4075-9d01-db5c26eedcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_folder(folder_path):\n",
    "    docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                docs.append(f.read())\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a83d07d3-5a98-4d97-80ca-1e963bc8e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def filter_chunks(chunks):\n",
    "    return [c for c in chunks if len(c.split()) >= 10]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ae81de1-670c-40d2-8922-8b7b158ab18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    if not chunks:\n",
    "        raise ValueError(\"No valid chunks found for vectorization.\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=None)\n",
    "    vectors = vectorizer.fit_transform(chunks)\n",
    "\n",
    "    if not vectorizer.vocabulary_:\n",
    "        raise ValueError(\"TF-IDF vocabulary is empty. Check your documents.\")\n",
    "\n",
    "    return vectors, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f66ccf4-31b2-4cff-b14e-d85293d62e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, chunks, vectorizer, chunk_vectors, top_k=3):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, chunk_vectors).flatten()\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [(chunks[i], similarities[i]) for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b84645a-be6d-467c-bd41-f8af204b7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without LLM\n",
    "def generate_answer(query, retrieved_chunks):\n",
    "    combined_context = ' '.join([chunk for chunk, _ in retrieved_chunks])\n",
    "    response = f\"Q: {query}\\nA (based on context): {combined_context[:500]}...\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b6f8c80-e4ce-4ae2-b38c-9f7a28957580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, docs_folder):\n",
    "    # Step 1: Load\n",
    "    docs = load_documents_from_folder(docs_folder)\n",
    "\n",
    "    # Step 2: Clean + Chunk + filtering\n",
    "    all_chunks = []\n",
    "    all_chunks = filter_chunks(chunk_text(clean_text(docs)))\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    chunk_vectors, vectorizer = create_embeddings(all_chunks)\n",
    "\n",
    "    # Step 4: Retrieval\n",
    "    top_chunks = retrieve(query, all_chunks, vectorizer, chunk_vectors)\n",
    "\n",
    "    # Step 5: Generation\n",
    "    answer = generate_answer(query, top_chunks)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "911308f3-2ec4-44af-97ed-8c023acefbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was Mr. Darcy's first impression of Elizabeth Bennet?\n",
      "A (based on context): fantastic limits and restrictions against which Mary Wollstonecraft protested with better general sense than particular taste or judgment. Miss Austen, too, drew back when the white frost touched her shoes; but I think she would have made a pretty good journey even in a black one._ _For if her knowledge was not very extended, she knew two things which only genius knows. The one was humanity, and the other was art. On the first head she could not make a mistake; her men, though limited, are true,...\n"
     ]
    }
   ],
   "source": [
    "docs_folder = \"/Users/mehuljain/Tests\"\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    query = \"What was Mr. Darcy's first impression of Elizabeth Bennet?\"\n",
    "    answer = rag_pipeline(query, docs_folder)\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfbdb4-555e-492c-9494-dfc76af02d57",
   "metadata": {},
   "source": [
    "## RAG with RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf4b1e50-16f3-41d4-9790-88781063c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Input Query]\n",
    "#      ↓\n",
    "# [Agent (RL Policy)]\n",
    "#      ↓\n",
    "# [Select Action]\n",
    "#      ↓\n",
    "# [Modified Query / Retrieval]\n",
    "#      ↓\n",
    "# [RAG Pipeline (Retrieval + Answer Generation)]\n",
    "#      ↓\n",
    "# [Answer]\n",
    "#      ↓\n",
    "# [Reward = BLEU Score vs Ground Truth]\n",
    "#      ↓\n",
    "# [Update Policy (optional)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c5a45b7-ed06-4bfe-8b83-baa6ba0a3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_query(query):\n",
    "    return query + \" in detail\"  # simplistic enhancement\n",
    "\n",
    "def expand_retrieval(chunks, top_chunks, k_extra=2):\n",
    "    # Add next best chunks not in top_chunks\n",
    "    top_indices = [chunks.index(c[0]) for c in top_chunks]\n",
    "    scores = [c[1] for c in top_chunks]\n",
    "    others = [(i, score) for i, score in enumerate(scores) if i not in top_indices]\n",
    "    others = sorted(others, key=lambda x: -x[1])[:k_extra]\n",
    "    extra_chunks = [(chunks[i], scores[i]) for i, _ in others]\n",
    "    return top_chunks + extra_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f91ed88d-878a-4ced-9e7f-d86586037370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, hypothesis):\n",
    "    bleu = sacrebleu.sentence_bleu(hypothesis, [reference])\n",
    "    return bleu.score / 100.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab3d3167-bb08-4fd2-8d38-cf8f9b95fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on max score\n",
    "def rl_decision_loop(query, ground_truth, docs_folder):\n",
    "    docs = load_documents_from_folder(docs_folder)\n",
    "\n",
    "    # Preprocessing\n",
    "    all_chunks = []\n",
    "    all_chunks = filter_chunks(chunk_text(clean_text(docs)))\n",
    "\n",
    "    chunk_vectors, vectorizer = create_embeddings(all_chunks)\n",
    "\n",
    "    # ACTION 1: Direct RAG\n",
    "    top_chunks_1 = retrieve(query, all_chunks, vectorizer, chunk_vectors)\n",
    "    answer_1 = generate_answer(query, top_chunks_1)\n",
    "    reward_1 = calculate_bleu(ground_truth, answer_1)\n",
    "\n",
    "    # ACTION 2: Improved Query\n",
    "    mod_query = improve_query(query)\n",
    "    top_chunks_2 = retrieve(mod_query, all_chunks, vectorizer, chunk_vectors)\n",
    "    answer_2 = generate_answer(mod_query, top_chunks_2)\n",
    "    reward_2 = calculate_bleu(ground_truth, answer_2)\n",
    "\n",
    "    # ACTION 3: Expanded Retrieval\n",
    "    top_chunks_3 = expand_retrieval(all_chunks, top_chunks_1)\n",
    "    answer_3 = generate_answer(query, top_chunks_3)\n",
    "    reward_3 = calculate_bleu(ground_truth, answer_3)\n",
    "\n",
    "    results = {\n",
    "        'direct': (answer_1, reward_1),\n",
    "        'query_improved': (answer_2, reward_2),\n",
    "        'retrieval_expanded': (answer_3, reward_3)\n",
    "    }\n",
    "\n",
    "    # Select best action based on reward\n",
    "    best_action = max(results.items(), key=lambda x: x[1][1])\n",
    "    print(f\"Best action: {best_action[0]} | Reward: {best_action[1][1]:.4f}\")\n",
    "    return best_action[1][0]  # best answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21e71a93-aacb-481c-a4f2-4f9f5b088789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best action: direct | Reward: 0.0043\n",
      "\n",
      "Final Answer:\n",
      " Q: What was Mr. Darcy's first impression of Elizabeth Bennet?\n",
      "A (based on context): fantastic limits and restrictions against which Mary Wollstonecraft protested with better general sense than particular taste or judgment. Miss Austen, too, drew back when the white frost touched her shoes; but I think she would have made a pretty good journey even in a black one._ _For if her knowledge was not very extended, she knew two things which only genius knows. The one was humanity, and the other was art. On the first head she could not make a mistake; her men, though limited, are true,...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"What was Mr. Darcy's first impression of Elizabeth Bennet?\"\n",
    "    ground_truth = \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water.\"\n",
    "\n",
    "    best_answer = rl_decision_loop(query, ground_truth, docs_folder)\n",
    "    print(\"\\nFinal Answer:\\n\", best_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f69256-b517-4fc0-92d1-09f9aec32bed",
   "metadata": {},
   "source": [
    "## RAG with Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a6be814-9af5-4cd6-a4d3-f861c2e9a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [\"direct\", \"query_improved\", \"retrieval_expanded\"]\n",
    "Q_TABLE = {}  # State-Action -> Q-value\n",
    "ALPHA = 0.1   # Learning rate\n",
    "GAMMA = 0.9   # Discount factor\n",
    "EPSILON = 0.2 # Exploration factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4531cb34-3491-4eed-b316-97f6f3d21865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(query):\n",
    "    return \"long\" if len(query.split()) > 5 else \"short\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08ef9b2c-aaf7-4fe7-bf35-6f51a5a47230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "    if random.random() < EPSILON or state not in q_table:\n",
    "        return random.choice(ACTIONS)\n",
    "    return max(q_table[state], key=q_table[state].get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8b34557-0ee8-4dad-abc4-52c29ae36c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state, action, reward, next_state, q_table):\n",
    "    if state not in q_table:\n",
    "        q_table[state] = {a: 0.0 for a in ACTIONS}\n",
    "    if next_state not in q_table:\n",
    "        q_table[next_state] = {a: 0.0 for a in ACTIONS}\n",
    "\n",
    "    max_future_q = max(q_table[next_state].values())\n",
    "    current_q = q_table[state][action]\n",
    "\n",
    "    new_q = current_q + ALPHA * (reward + GAMMA * max_future_q - current_q)\n",
    "    q_table[state][action] = new_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "11be9012-962c-4606-80ed-9dc0cde4e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_rag(queries, ground_truths, docs_folder, episodes=100):\n",
    "    global Q_TABLE\n",
    "\n",
    "    docs = load_documents_from_folder(docs_folder)\n",
    "    all_chunks = []\n",
    "    all_chunks = filter_chunks(chunk_text(clean_text(docs)))\n",
    "        \n",
    "    chunk_vectors, vectorizer = create_embeddings(all_chunks)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        total_reward = 0\n",
    "\n",
    "        for query, gt in zip(queries, ground_truths):\n",
    "            state = get_state(query)\n",
    "            action = choose_action(state, Q_TABLE)\n",
    "\n",
    "            if action == \"query_improved\":\n",
    "                mod_query = improve_query(query)\n",
    "                top_chunks = retrieve(mod_query, all_chunks, vectorizer, chunk_vectors)\n",
    "                answer = generate_answer(mod_query, top_chunks)\n",
    "\n",
    "            elif action == \"retrieval_expanded\":\n",
    "                top_chunks_direct = retrieve(query, all_chunks, vectorizer, chunk_vectors)\n",
    "                top_chunks = expand_retrieval(all_chunks, top_chunks_direct)\n",
    "                answer = generate_answer(query, top_chunks)\n",
    "\n",
    "            else: \n",
    "                top_chunks = retrieve(query, all_chunks, vectorizer, chunk_vectors)\n",
    "                answer = generate_answer(query, top_chunks)\n",
    "\n",
    "            reward = calculate_bleu(gt, answer)\n",
    "            next_state = get_state(query)  \n",
    "\n",
    "            update_q_table(state, action, reward, next_state, Q_TABLE)\n",
    "            total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "39a22700-a505-4b6a-9310-9568ab4558a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'short': {'direct': 0.0030900863822670723,\n",
       "  'query_improved': 0.003523775362974809,\n",
       "  'retrieval_expanded': 0.003127519187687356}}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9441d3f5-30ff-4569-aa7c-a7e56a62f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_best_action(query, docs_folder):\n",
    "    state = get_state(query)\n",
    "\n",
    "    if state not in Q_TABLE:\n",
    "        action = random.choice(ACTIONS)\n",
    "    else:\n",
    "        action = max(Q_TABLE[state], key=Q_TABLE[state].get)\n",
    "\n",
    "    docs = load_documents_from_folder(docs_folder)\n",
    "    all_chunks = []\n",
    "    all_chunks = filter_chunks(chunk_text(clean_text(docs)))\n",
    "        \n",
    "    chunk_vectors, vectorizer = create_embeddings(all_chunks)\n",
    "\n",
    "    if action == \"query_improved\":\n",
    "        mod_query = improve_query(query)\n",
    "        top_chunks = retrieve(mod_query, all_chunks, vectorizer, chunk_vectors)\n",
    "        answer = generate_answer(mod_query, top_chunks)\n",
    "\n",
    "    elif action == \"retrieval_expanded\":\n",
    "        top_chunks_direct = retrieve(query, all_chunks, vectorizer, chunk_vectors)\n",
    "        top_chunks = expand_retrieval(all_chunks, top_chunks_direct)\n",
    "        answer = generate_answer(query, top_chunks)\n",
    "\n",
    "    else: \n",
    "        top_chunks = retrieve(query, all_chunks, vectorizer, chunk_vectors)\n",
    "        answer = generate_answer(query, top_chunks)\n",
    "\n",
    "    print(f\"Inference used action: {action}\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d7c1ce2a-7eed-41a9-9872-bbf9a33ce94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference used action: direct\n",
      "\n",
      "Final Answer:\n",
      " Q: What was Mr. Darcy's first impression of Elizabeth Bennet?\n",
      "A (based on context): fantastic limits and restrictions against which Mary Wollstonecraft protested with better general sense than particular taste or judgment. Miss Austen, too, drew back when the white frost touched her shoes; but I think she would have made a pretty good journey even in a black one._ _For if her knowledge was not very extended, she knew two things which only genius knows. The one was humanity, and the other was art. On the first head she could not make a mistake; her men, though limited, are true,...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"What was Mr. Darcy's first impression of Elizabeth Bennet?\"\n",
    "    ground_truth = \"\"\"Mr. Darcy initially considered Elizabeth Bennet \"tolerable, but not handsome enough to tempt me.\" This remark reflected his pride and set the tone for their evolving relationship throughout the novel.\"\"\"\n",
    "    \n",
    "    train_rl_rag(query, ground_truth, docs_folder, episodes=10)\n",
    "    best_answer = infer_best_action(query, docs_folder)\n",
    "    print(\"\\nFinal Answer:\\n\", best_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a534cb8-a1b9-444f-ab54-7e84a1b131f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
