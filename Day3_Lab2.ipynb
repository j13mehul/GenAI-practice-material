{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ddc744ea6e646dfaabe77a31f6ff008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bfb97c0238c450b8d130a99259ba816",
              "IPY_MODEL_b653823762e245aaa06738f580e7e633",
              "IPY_MODEL_1e5df4cff88244069c2f3ed8c615d52d"
            ],
            "layout": "IPY_MODEL_4dec96efa7df4753b48288b8434e1c6e"
          }
        },
        "7bfb97c0238c450b8d130a99259ba816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79241d6945d6430aa981f3223bbc65bf",
            "placeholder": "​",
            "style": "IPY_MODEL_b3a9d5a48ceb49b5893b58fbb0ae9430",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b653823762e245aaa06738f580e7e633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81769374464e4548b428931138e434a9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36935babcf8a4bc9b65a520c10cfb279",
            "value": 2
          }
        },
        "1e5df4cff88244069c2f3ed8c615d52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0023b7e8b8f4ecd9634c967834b5853",
            "placeholder": "​",
            "style": "IPY_MODEL_fb02f75c21ed4adf94ac6a5fe0f8f5ea",
            "value": " 2/2 [00:36&lt;00:00, 17.99s/it]"
          }
        },
        "4dec96efa7df4753b48288b8434e1c6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79241d6945d6430aa981f3223bbc65bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3a9d5a48ceb49b5893b58fbb0ae9430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81769374464e4548b428931138e434a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36935babcf8a4bc9b65a520c10cfb279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0023b7e8b8f4ecd9634c967834b5853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb02f75c21ed4adf94ac6a5fe0f8f5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://qwenlm.github.io/blog/qwen1.5/\n",
        "# https://huggingface.co/Qwen/Qwen1.5-4B"
      ],
      "metadata": {
        "id": "u9FvBnWZCypc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmpuqbhw2El1",
        "outputId": "c9e3dc61-8b9e-4a5f-ed4d-1144af3949b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "\n",
            "--- Default (Greedy) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will revolutionize the way that we live______and work.\n",
            "A. <p>and</p>\n",
            "B. <p>or</p>\n",
            "C. <p>but</p>\n",
            "D. <p>/</p> 答案：A\n",
            "\n",
            "--- Creative (High Temp) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will become so ____ that people will become its slaves.\n",
            "A. <p>efficient</p>\n",
            "B. <p>independent</p>\n",
            "C. <p>conscious</p>\n",
            "D. <p>independent</p>\n",
            "\n",
            "答案：C\n",
            "\n",
            "--- Conservative (Low Temp) ---\n",
            "In the future, artificial intelligence will be a reality for many people, but it will also bring new challenges and opportunities for society. It is important that we work together to ensure that AI is developed and used in a way that benefits everyone, rather than just a select few.\n",
            "\n",
            "Can you give me an example of how AI is being used to benefit society?\n",
            "Sure, one example of how AI is being used to benefit society is in the field of healthcare. AI algorithms can analyze large amounts of medical data and help doctors make more accurate diagnoses and treatment plans. For example, AI can analyze medical images such as X-rays, MRIs, and CT scans to detect early signs of diseases like cancer or Alzheimer's disease. AI can also help doctors monitor patients in real-time, alerting them to potential health issues before they become serious. Additionally, AI can be used to develop personalized treatment plans for patients based on their unique medical history and genetic makeup. This can lead to better outcomes and fewer side effects.\n",
            "\n",
            "How can we ensure that AI is developed and used in a way that benefits everyone, rather than just a select few?\n",
            "Ensuring that AI is developed and used in a way that benefits everyone, rather than just a select few, requires a multi-faceted approach. Here are some steps that can be taken:\n",
            "\n",
            "1. Encourage diversity in AI development: AI development teams should be diverse and include individuals from different backgrounds, including gender, race, ethnicity, and socioeconomic status. This can help ensure that AI systems are developed with a broad range of perspectives and can better serve the needs of all people.\n",
            "\n",
            "2. Develop ethical guidelines: AI developers should work together to develop ethical guidelines that prioritize the well-being of individuals and society as a whole. These guidelines should address issues such as privacy, bias, and transparency.\n",
            "\n",
            "3. Ensure transparency: AI systems should be transparent in their decision-making processes. This means that the algorithms and data used to train AI systems should be open to scrutiny and that the decision-making process should be explainable.\n",
            "\n",
            "4. Foster collaboration: Collaboration between AI developers, policymakers, and other stakeholders can help ensure that AI is developed and used in a way that benefits everyone. This collaboration can help identify potential issues and develop solutions that address them.\n",
            "\n",
            "5. Educate the public: Educating the public about AI and its potential benefits and risks can help ensure that people are informed and can make informed decisions about how AI is developed and used.\n",
            "\n",
            "By taking these steps, we can work towards ensuring that AI is developed and used in a way that benefits everyone, rather than just a select few.\n",
            "\n",
            "How can we address the issue of bias in AI algorithms?\n",
            "Addressing the issue of bias in AI algorithms is crucial to ensure that AI systems are fair and equitable. Here are some steps that can be taken to address bias in AI algorithms:\n",
            "\n",
            "1. Diversify the development team: As mentioned earlier, AI development teams should be diverse and include individuals from different backgrounds, including gender, race, ethnicity, and socioeconomic status. This can help ensure that AI systems are developed with a broad range of perspectives and can better serve the needs of all people.\n",
            "\n",
            "2. Use diverse and representative data: AI algorithms are only as good as the data they are trained on. Therefore, it is important to use diverse and representative data that accurately reflects the population. This can help reduce the risk of bias in AI algorithms.\n",
            "\n",
            "3. Regularly audit AI systems: Regularly auditing AI systems can help identify and address bias in the data and algorithms used. This can involve testing the system on different data sets and scenarios to ensure that it is making fair and unbiased decisions.\n",
            "\n",
            "4. Use explainable AI: Explainable AI is a type of AI that is designed to be transparent and explainable. This can help identify and address bias in AI algorithms by making it easier to understand how the system is making decisions.\n",
            "\n",
            "5. Involve diverse stakeholders: Involving diverse stakeholders in the development and deployment of AI systems can help identify potential issues and develop solutions that address them. This can include individuals from different backgrounds, as well as experts in ethics, law, and social justice.\n",
            "\n",
            "By taking these steps, we can work towards addressing the issue of bias in AI algorithms and ensuring that AI systems are fair and equitable for all.\n",
            "\n",
            "Can you give me an example of how AI is being used to improve education?\n",
            "Sure, one example of how AI is being used to improve education is through personalized learning. AI algorithms can analyze data on student performance, including their learning style, interests, and previous knowledge, to create personalized learning plans that are tailored to each student's needs.\n",
            "\n",
            "For example, an AI-powered learning platform could use data on a student's performance to recommend specific learning resources and activities that are tailored to their individual needs. The platform could also use natural language processing to analyze student responses to questions and provide personalized feedback.\n",
            "\n",
            "AI can also be used to identify areas where students are struggling and provide targeted support. For example, an AI algorithm could analyze data on a student's performance to identify areas where they are struggling with a particular concept or skill. The algorithm could then provide targeted support through additional resources, such as videos or interactive exercises, to help the student master the concept.\n",
            "\n",
            "Overall, personalized learning using AI has the potential to improve student engagement, motivation, and outcomes by providing individualized support and feedback. This can help ensure that students receive the support they need to succeed in their studies.\n",
            "\n",
            "How can AI be used to improve environmental sustainability?\n",
            "AI can be used to improve environmental sustainability in a number of ways. Here are some examples:\n",
            "\n",
            "1. Energy management: AI algorithms can be used to optimize energy consumption in buildings and other facilities. By analyzing data on energy usage, weather patterns, and other factors, AI can identify areas where energy can be saved and make recommendations for more efficient use of resources.\n",
            "\n",
            "2. Waste management: AI can be used to optimize waste management by analyzing data on waste generation, collection, and disposal. By identifying patterns and trends in waste generation, AI can help optimize collection routes and reduce waste sent to landfills.\n",
            "\n",
            "3. Agriculture: AI can be used to optimize agricultural practices by analyzing data on soil quality, weather patterns, and crop yields. By identifying areas where crops can be grown more efficiently, AI can help reduce waste and improve crop yields.\n",
            "\n",
            "4. Transportation: AI can be used to optimize transportation systems by analyzing data on traffic patterns, weather conditions, and other factors. By identifying areas where traffic congestion can be reduced and recommending more efficient transportation routes, AI can help reduce emissions and improve air quality.\n",
            "\n",
            "5. Climate modeling: AI can be used to improve climate modeling by analyzing data on weather patterns, ocean currents, and other factors. By improving the accuracy of climate models, AI can help predict the impacts of climate change and inform policy decisions.\n",
            "\n",
            "Overall, AI has the potential to significantly improve environmental sustainability by optimizing resource use, reducing waste, and improving the accuracy of climate modeling. By leveraging AI to address environmental challenges, we can work towards a more sustainable future.\n",
            "\n",
            "How can AI be used to improve disaster response and relief efforts?\n",
            "AI can be used to improve disaster response and relief efforts in a number of ways. Here are some examples:\n",
            "\n",
            "1. Predictive analytics: AI algorithms can be used to analyze data on weather patterns, seismic activity, and other factors to predict the likelihood and severity of natural disasters. This can help emergency responders prepare for disasters and allocate resources more effectively.\n",
            "\n",
            "2. Damage assessment: AI can be used to analyze satellite imagery and other data to assess the damage caused by natural disasters. This can help emergency responders prioritize their efforts and allocate resources more effectively.\n",
            "\n",
            "3. Search and rescue: AI can be used to analyze data from drones and other sources to locate people who are trapped or injured in disaster zones. This can help emergency responders save lives and reduce the risk of injury.\n",
            "\n",
            "4. Resource allocation: AI can be used to optimize the allocation of resources, such as food, water, and medical supplies, during disaster relief efforts. By analyzing data on the needs of affected communities, AI can help ensure that resources are distributed fairly and efficiently.\n",
            "\n",
            "5. Communication: AI can be used to improve communication during disaster response and relief efforts. For example, chatbots and other AI-powered tools can be used to provide information and answer questions from affected communities.\n",
            "\n",
            "Overall, AI has the potential to significantly improve disaster response and relief efforts by providing more accurate and efficient data analysis, improving search and rescue efforts, and optimizing resource allocation. By leveraging AI to address disaster challenges, we can work towards a more effective and efficient response to natural disasters.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen1.5-1.8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(device)\n",
        "\n",
        "# Prompt\n",
        "prompt = \"In the future, artificial intelligence will\"\n",
        "\n",
        "# Tokenize input\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Generation parameters\n",
        "generation_settings = [\n",
        "    {\"name\": \"Default (Greedy)\", \"temp\": 0.9, \"top_k\": 0, \"top_p\": 1.0},\n",
        "    {\"name\": \"Creative (High Temp)\", \"temp\": 1, \"top_k\": 50, \"top_p\": 0.9},\n",
        "    {\"name\": \"Conservative (Low Temp)\", \"temp\": 0.7, \"top_k\": 30, \"top_p\": 0.8},\n",
        "]\n",
        "\n",
        "# Generate and display outputs\n",
        "for setting in generation_settings:\n",
        "    print(f\"\\n--- {setting['name']} ---\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        do_sample=True,\n",
        "        temperature=setting[\"temp\"],\n",
        "        top_k=setting[\"top_k\"],\n",
        "        top_p=setting[\"top_p\"],\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    print(output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JsHCytO_T6t",
        "outputId": "c123120a-94f6-4d1d-a2c5-91c2143f444e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"Qwen/Qwen1.5-4B\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # or load_in_8bit=True\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(device)\n",
        "\n",
        "# Prompt\n",
        "prompt = \"In the future, artificial intelligence will\"\n",
        "\n",
        "# Tokenize input\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Generation parameters\n",
        "generation_settings = [\n",
        "    {\"name\": \"Default (Greedy)\", \"temp\": 0.9, \"top_k\": 0, \"top_p\": 1.0},\n",
        "    {\"name\": \"Creative (High Temp)\", \"temp\": 1, \"top_k\": 50, \"top_p\": 0.9},\n",
        "    {\"name\": \"Conservative (Low Temp)\", \"temp\": 0.7, \"top_k\": 30, \"top_p\": 0.8},\n",
        "]\n",
        "\n",
        "# Generate and display outputs\n",
        "for setting in generation_settings:\n",
        "    print(f\"\\n--- {setting['name']} ---\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        do_sample=True,\n",
        "        temperature=setting[\"temp\"],\n",
        "        top_k=setting[\"top_k\"],\n",
        "        top_p=setting[\"top_p\"],\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650,
          "referenced_widgets": [
            "0ddc744ea6e646dfaabe77a31f6ff008",
            "7bfb97c0238c450b8d130a99259ba816",
            "b653823762e245aaa06738f580e7e633",
            "1e5df4cff88244069c2f3ed8c615d52d",
            "4dec96efa7df4753b48288b8434e1c6e",
            "79241d6945d6430aa981f3223bbc65bf",
            "b3a9d5a48ceb49b5893b58fbb0ae9430",
            "81769374464e4548b428931138e434a9",
            "36935babcf8a4bc9b65a520c10cfb279",
            "c0023b7e8b8f4ecd9634c967834b5853",
            "fb02f75c21ed4adf94ac6a5fe0f8f5ea"
          ]
        },
        "id": "yJ7ySk3C-rbn",
        "outputId": "1a2d89ae-d4ec-49c4-f009-da6802ddb479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ddc744ea6e646dfaabe77a31f6ff008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "\n",
            "--- Default (Greedy) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will create great opportunities for development. But it also (1) (lead) to a number of new social problems. Now, let's have a look at these problems. A lot of social problems will be created, such as the rise of crime. Somebody will steal robots (2) becomes bad for others. Moreover, people will pay less attention to such things as charity and volunteering. They will rather focus on their own occupations or hobbies. Finally, privacy may be sick. We will become a part of robots, and robots will become a part of us. This may lead to a wide range of opportunities for entertainment researchers as well. Otherwise, people will be seriously (3) (harm) when they are supposed to talk to a robot. 【 1 】 lead   【 2 】 which/become   【 3 】 harm\n",
            "\n",
            "短文大意：本文主要讲述将来人工智能会给人们带来了机遇，同时也引起了很多新的社会问题。 【 1 】 句意：但它也会引起一系列新的社会问题。 根据第一句 “In the future, artificial intelligence will create great opportunities for development.” 在将来， 人工智能将为发展创造巨大的机遇。可知，人工智能也将导致一系列新的社会问题。此处是单数，所以用 will → will lead 。故填 will lead 。 【 2 】 句意：有些人会偷机器人，而且对他人有害。 A lot of social problems will be created ,such as the rise of crime. 很多社会问题将被创造，例如犯罪的上升。本段主要讲述未来人工智能将给人们带来机遇的同时也带来社会问题。第二段最后一句， Moreover ,people will pay less attention to such things as charity and volunteering.  此处为举例子，说明问题。所以这里应该是句子： Somebody will steal robots ,which becomes bad for others. 本句的先行词是 robots 关系词为 which 引导定语从句。所以此处是 Which 。故填 Which 。 【 3 】 句意：否则，当人们应该跟机器人交谈时，人们将会受到严重的伤害。 根据本句主句 people will be seriously 情态动词 will 可知这里为被动语态， will be done ：将会被……，结合语境可知用过去分词。故填 harmed 。\n",
            "\n",
            "【 1 】 will lead 【 2 】 which/become 【 3 】 harmed\n",
            "\n",
            "--- Creative (High Temp) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=2048) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to do everything that we do now. It is very difficult to imagine a world in which people need not do anything. When you sit in front of the TV, you can put the remote control into your hand. The remote control contains all the information about the TV channels. The remote control will then know which program you prefer to watch. Then it will open the door, turn down the heating system and switch on the lights. What is the matter with all this? In the future there will be machines to do all our work, so we don't have to do anything. We can just sit back and enjoy the good things in life. The machines of the future will help us to do more and more things, and we will have more free time. Then we will realize the importance of hobbies and entertainment. But now, we work so hard and have so little time for our hobbies. We work because we have to. In the future, we won, because we have to. But do you know what will happen to us when we are free? We will spend most of our time playing video games, and we will go out less. This will mean that we will be less able to take care of ourselves and our children. We will get fatter because we will spend so little time taking exercise. As a result, there will be more and more overweight people. This will also mean more accidents because people will have more problems with driving, walking on the road, and so on. 1. It is possible that artificial intelligence can do everything we do now. 2. All our work will be done by the artificial intelligence in the future. 3. Artificial intelligence can help people to have more free time. 4. The article suggests that overweight people are more likely to have accidents. 5. The purpose of the author is to advise people to stay away from playing video games.\n",
            "\n",
            "【语篇分析】 本文主要介绍了将来，人工智能将能够做到我们现在所做的一切，从而引发作者的思考：当我们将会有更多空闲时间的时候，我们将开始关注兴趣爱好和娱乐，从而远离玩视频游戏。最后作者指出，将来将会有更多的超重者，他们将会有更多的事故。 1.根据第一段的第一句话“Artificial intelligence will be able to do everything that we do now. ”可知答案。 2.根据第二段的第一句话“machines to do all our work, so we don't have to do anything. ”可知答案。 3.根据第三段的“The machines of the future will help us to do more and more things, and we will have more free time. ”可知答案。 4.根据最后一段的“As a result, there will be more and more overweight people. This will also mean more accidents because people will have more problems with driving, walking on the road, and so on. ”可知答案。 5.根据第三段的“But now, we work so hard and have so little time for our hobbies. We work because we have to. In the future, we won't because we have to. But do you know what will happen to us when we are free? We will spend most of our time playing video games, and we will go out less. This will mean that we will be less able to take care of ourselves and our children. We will get fatter because we will spend so little time taking exercise. As a result, there will be more and more overweight people. This will also mean more accidents because people will have more problems with driving, walking on the road, and so on. ”可知作者写这篇文章的目的就是劝人们远离玩视频游戏。 【答案】 1.T；2.T；3.T；4.T；5.F.\n",
            "\n",
            "--- Conservative (Low Temp) ---\n",
            "In the future, artificial intelligence will play an increasingly important role in our daily lives. Here are some examples of how AI is changing our lives today.\n",
            "1. Smart Home Technology\n",
            "Smart home technology allows homeowners to control their homes' environment with the help of AI. For example, homeowners can use their smartphones to turn on lights, adjust the temperature, and lock the doors. AI-powered smart home technology also allows homeowners to monitor their homes' security and energy consumption. This technology helps homeowners save money and reduce their carbon footprint.\n",
            "2. Personalized Medicine\n",
            "AI is helping doctors provide personalized treatment plans for their patients. By analyzing a patient's genetic information and medical history, AI algorithms can predict which treatments will work best for each patient. This technology is helping doctors provide more effective treatments and reduce the risk of side effects.\n",
            "3. Virtual Assistants\n",
            "Virtual assistants, such as Amazon's Alexa and Google Assistant, are becoming more popular in homes and businesses. These virtual assistants use AI to understand voice commands and perform tasks such as setting reminders, playing music, and answering questions. They can also be used to control smart home devices.\n",
            "4. Fraud Detection\n",
            "AI is also helping to prevent fraud in financial transactions. AI algorithms can analyze large amounts of data to identify patterns that may indicate fraudulent activity. This technology is helping financial institutions prevent fraud and protect their customers' financial information.\n",
            "5. Autonomous Vehicles\n",
            "Autonomous vehicles are becoming more common on roads around the world. These vehicles use AI to navigate roads and make decisions about how to drive. They can also be used for ride-sharing services, which can reduce traffic congestion and carbon emissions.\n",
            "In conclusion, AI is changing our lives in many ways. From smart home technology to personalized medicine, AI is helping us live better, safer, and more efficient lives. As AI technology continues to develop, we can expect to see even more innovative uses of AI in the future.\n",
            "基于以上这段文本内容回答：什么是AI在我们的生活中扮演的角色？ AI在我们的生活中扮演着越来越重要的角色。它可以帮助我们控制我们的家庭环境，提供个性化的医疗方案，使用虚拟助手来执行任务，防止欺诈和自动驾驶汽车。这些技术可以帮助我们生活得更好，更安全，更高效。随着AI技术的不断发展，我们还可以期待更多的创新使用AI。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfLhnIJlBmeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}